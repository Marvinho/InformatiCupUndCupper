@article{journals/corr/SzegedyZSBEGF13,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
	biburl = {https://www.bibsonomy.org/bibtex/283a0e880777f36959760992d9d8eb1fb/dblp},
	ee = {http://arxiv.org/abs/1312.6199},
	interhash = {8be4106d034bcba43126e1ce32fd0b2a},
	intrahash = {83a0e880777f36959760992d9d8eb1fb},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:22:34.000+0200},
	title = {Intriguing properties of neural networks.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html#SzegedyZSBEGF13},
	volume = {abs/1312.6199},
	year = 2013
}

@article{journals/corr/KurakinGB16a,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Kurakin, Alexey and Goodfellow, Ian J. and Bengio, Samy},
	biburl = {https://www.bibsonomy.org/bibtex/29a4be95014be8d48f26700960b7c7c4d/dblp},
	ee = {http://arxiv.org/abs/1611.01236},
	interhash = {9ef7700fc706ce95cce528c6535f4ae1},
	intrahash = {9a4be95014be8d48f26700960b7c7c4d},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:19:40.000+0200},
	title = {Adversarial Machine Learning at Scale.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1611.html#KurakinGB16a},
	volume = {abs/1611.01236},
	year = 2016
}

@article{journals/corr/PapernotMG16,
	added-at = {2016-11-17T00:00:00.000+0100},
	author = {Papernot, Nicolas and McDaniel, Patrick D. and Goodfellow, Ian J.},
	biburl = {https://www.bibsonomy.org/bibtex/2ff60c031c6a1d207e6ad668bc6035202/dblp},
	ee = {http://arxiv.org/abs/1605.07277},
	interhash = {41a38aa24dba894a5dd1bce4619289d3},
	intrahash = {ff60c031c6a1d207e6ad668bc6035202},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2016-11-19T11:34:15.000+0100},
	title = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1605.html#PapernotMG16},
	volume = {abs/1605.07277},
	year = 2016
}

@misc{kurakin2016adversarial,
	abstract = {Most existing machine learning classifiers are highly vulnerable to
	adversarial examples. An adversarial example is a sample of input data which
	has been modified very slightly in a way that is intended to cause a machine
	learning classifier to misclassify it. In many cases, these modifications can
	be so subtle that a human observer does not even notice the modification at
	all, yet the classifier still makes a mistake. Adversarial examples pose
	security concerns because they could be used to perform an attack on machine
	learning systems, even if the adversary has no access to the underlying model.
	Up to now, all previous work have assumed a threat model in which the adversary
	can feed data directly into the machine learning classifier. This is not always
	the case for systems operating in the physical world, for example those which
	are using signals from cameras and other sensors as an input. This paper shows
	that even in such physical world scenarios, machine learning systems are
	vulnerable to adversarial examples. We demonstrate this by feeding adversarial
	images obtained from cell-phone camera to an ImageNet Inception classifier and
	measuring the classification accuracy of the system. We find that a large
	fraction of adversarial examples are classified incorrectly even when perceived
	through the camera.},
	added-at = {2017-04-23T14:36:36.000+0200},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	biburl = {https://www.bibsonomy.org/bibtex/2073ed4281732090b3187957cd520307d/albinzehe},
	description = {Adversarial examples in the physical world},
	interhash = {361d554f9ed25cedc9b747f1588f5a69},
	intrahash = {073ed4281732090b3187957cd520307d},
	keywords = {adversarial neuralnet thema thema:adversarial-examples},
	note = {cite arxiv:1607.02533Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk},
	timestamp = {2017-04-23T14:37:15.000+0200},
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	year = 2016
}

@article{goodfellow2014explaining,
	added-at = {2017-05-07T15:43:59.000+0200},
	author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	biburl = {https://www.bibsonomy.org/bibtex/23abd27c65ba96a3af0cdc1e4c8be7452/joachimagne},
	interhash = {cfd4056e0a17212d539d2ef112c60daf},
	intrahash = {3abd27c65ba96a3af0cdc1e4c8be7452},
	journal = {arXiv preprint arXiv:1412.6572},
	keywords = {thema:adversarial},
	timestamp = {2017-05-09T08:57:14.000+0200},
	title = {Explaining and harnessing adversarial examples},
	year = 2014
}

@article{journals/corr/abs-1802-01421,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Schölkopf, Bernhard and Bottou, Léon and Lopez-Paz, David},
	biburl = {https://www.bibsonomy.org/bibtex/2e7978b30d031df3574eb7464a97f01b6/dblp},
	ee = {http://arxiv.org/abs/1802.01421},
	interhash = {9485fa7024a12d0c3af01cd8ecaf723e},
	intrahash = {e7978b30d031df3574eb7464a97f01b6},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T13:55:34.000+0200},
	title = {Adversarial Vulnerability of Neural Networks Increases With Input Dimension.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-01421},
	volume = {abs/1802.01421},
	year = 2018
}

@inproceedings{conf/ccs/PapernotMGJCS17,
	added-at = {2018-11-06T00:00:00.000+0100},
	author = {Papernot, Nicolas and McDaniel, Patrick D. and Goodfellow, Ian J. and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	biburl = {https://www.bibsonomy.org/bibtex/29677bbc5d6ff7494dad69bf98d1546e1/dblp},
	booktitle = {AsiaCCS},
	crossref = {conf/ccs/2017asia},
	editor = {Karri, Ramesh and Sinanoglu, Ozgur and Sadeghi, Ahmad-Reza and Yi, Xun},
	ee = {https://doi.org/10.1145/3052973.3053009},
	interhash = {a0523787a7e7d1f1c11eccd279e1bd67},
	intrahash = {9677bbc5d6ff7494dad69bf98d1546e1},
	isbn = {978-1-4503-4944-4},
	keywords = {dblp},
	pages = {506-519},
	publisher = {ACM},
	timestamp = {2018-11-07T12:50:06.000+0100},
	title = {Practical Black-Box Attacks against Machine Learning.},
	url = {http://dblp.uni-trier.de/db/conf/ccs/asiaccs2017.html#PapernotMGJCS17},
	year = 2017
}

@inproceedings{conf/ijcai/XiaoLZHLS18,
	added-at = {2018-08-20T00:00:00.000+0200},
	author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
	biburl = {https://www.bibsonomy.org/bibtex/2da9c38a93183ac4322900e78aaa9d222/dblp},
	booktitle = {IJCAI},
	crossref = {conf/ijcai/2018},
	editor = {Lang, Jérôme},
	ee = {https://doi.org/10.24963/ijcai.2018/543},
	interhash = {81d73507b0cfb1af35979bd13e335e8e},
	intrahash = {da9c38a93183ac4322900e78aaa9d222},
	isbn = {978-0-9992411-2-7},
	keywords = {dblp},
	pages = {3905-3911},
	publisher = {ijcai.org},
	timestamp = {2018-08-21T11:39:25.000+0200},
	title = {Generating Adversarial Examples with Adversarial Networks.},
	url = {http://dblp.uni-trier.de/db/conf/ijcai/ijcai2018.html#XiaoLZHLS18},
	year = 2018
}

@article{journals/corr/HintonVD15,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeffrey},
	biburl = {https://www.bibsonomy.org/bibtex/2e1fba5d26dcac3592c8f91d7603a3e16/dblp},
	ee = {http://arxiv.org/abs/1503.02531},
	interhash = {67f341022f752a5833b5c6c35903c111},
	intrahash = {e1fba5d26dcac3592c8f91d7603a3e16},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:13:51.000+0200},
	title = {Distilling the Knowledge in a Neural Network.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1503.html#HintonVD15},
	volume = {abs/1503.02531},
	year = 2015
}


\\
